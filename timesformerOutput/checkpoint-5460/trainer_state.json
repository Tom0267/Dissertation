{
  "best_global_step": 5460,
  "best_metric": 0.8698630136986302,
  "best_model_checkpoint": "./timesformerOutput/checkpoint-5460",
  "epoch": 4.0,
  "eval_steps": 500,
  "global_step": 5460,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.014652014652014652,
      "grad_norm": 0.4818078875541687,
      "learning_rate": 4.994139194139195e-05,
      "loss": 1.6312,
      "step": 20
    },
    {
      "epoch": 0.029304029304029304,
      "grad_norm": 25.354820251464844,
      "learning_rate": 4.9868131868131877e-05,
      "loss": 1.7603,
      "step": 40
    },
    {
      "epoch": 0.04395604395604396,
      "grad_norm": 37.94664001464844,
      "learning_rate": 4.979487179487179e-05,
      "loss": 1.5277,
      "step": 60
    },
    {
      "epoch": 0.05860805860805861,
      "grad_norm": 2.764453411102295,
      "learning_rate": 4.972161172161172e-05,
      "loss": 1.1805,
      "step": 80
    },
    {
      "epoch": 0.07326007326007326,
      "grad_norm": 3.0912246704101562,
      "learning_rate": 4.964835164835165e-05,
      "loss": 1.1376,
      "step": 100
    },
    {
      "epoch": 0.08791208791208792,
      "grad_norm": 28.583614349365234,
      "learning_rate": 4.957509157509158e-05,
      "loss": 1.3123,
      "step": 120
    },
    {
      "epoch": 0.10256410256410256,
      "grad_norm": 14.96668815612793,
      "learning_rate": 4.9501831501831506e-05,
      "loss": 1.1887,
      "step": 140
    },
    {
      "epoch": 0.11721611721611722,
      "grad_norm": 0.37448936700820923,
      "learning_rate": 4.942857142857143e-05,
      "loss": 0.802,
      "step": 160
    },
    {
      "epoch": 0.13186813186813187,
      "grad_norm": 38.628173828125,
      "learning_rate": 4.9355311355311356e-05,
      "loss": 1.5789,
      "step": 180
    },
    {
      "epoch": 0.14652014652014653,
      "grad_norm": 9.106725692749023,
      "learning_rate": 4.9282051282051285e-05,
      "loss": 1.6575,
      "step": 200
    },
    {
      "epoch": 0.16117216117216118,
      "grad_norm": 1.916675090789795,
      "learning_rate": 4.9208791208791213e-05,
      "loss": 1.0251,
      "step": 220
    },
    {
      "epoch": 0.17582417582417584,
      "grad_norm": 27.504653930664062,
      "learning_rate": 4.9135531135531135e-05,
      "loss": 1.1401,
      "step": 240
    },
    {
      "epoch": 0.19047619047619047,
      "grad_norm": 33.05335998535156,
      "learning_rate": 4.9062271062271064e-05,
      "loss": 1.5735,
      "step": 260
    },
    {
      "epoch": 0.20512820512820512,
      "grad_norm": 26.66966438293457,
      "learning_rate": 4.898901098901099e-05,
      "loss": 1.5028,
      "step": 280
    },
    {
      "epoch": 0.21978021978021978,
      "grad_norm": 19.220170974731445,
      "learning_rate": 4.891575091575092e-05,
      "loss": 1.5297,
      "step": 300
    },
    {
      "epoch": 0.23443223443223443,
      "grad_norm": 25.81643295288086,
      "learning_rate": 4.884249084249084e-05,
      "loss": 0.8983,
      "step": 320
    },
    {
      "epoch": 0.2490842490842491,
      "grad_norm": 0.2678162157535553,
      "learning_rate": 4.876923076923077e-05,
      "loss": 1.5874,
      "step": 340
    },
    {
      "epoch": 0.26373626373626374,
      "grad_norm": 0.6423011422157288,
      "learning_rate": 4.86959706959707e-05,
      "loss": 1.0482,
      "step": 360
    },
    {
      "epoch": 0.2783882783882784,
      "grad_norm": 7.710428714752197,
      "learning_rate": 4.862271062271063e-05,
      "loss": 0.8896,
      "step": 380
    },
    {
      "epoch": 0.29304029304029305,
      "grad_norm": 43.10382843017578,
      "learning_rate": 4.854945054945055e-05,
      "loss": 0.9408,
      "step": 400
    },
    {
      "epoch": 0.3076923076923077,
      "grad_norm": 42.20844268798828,
      "learning_rate": 4.847619047619048e-05,
      "loss": 2.0638,
      "step": 420
    },
    {
      "epoch": 0.32234432234432236,
      "grad_norm": 44.782222747802734,
      "learning_rate": 4.840293040293041e-05,
      "loss": 1.7228,
      "step": 440
    },
    {
      "epoch": 0.336996336996337,
      "grad_norm": 9.963167190551758,
      "learning_rate": 4.8329670329670336e-05,
      "loss": 1.0237,
      "step": 460
    },
    {
      "epoch": 0.3516483516483517,
      "grad_norm": 7.937928676605225,
      "learning_rate": 4.825641025641026e-05,
      "loss": 0.7069,
      "step": 480
    },
    {
      "epoch": 0.3663003663003663,
      "grad_norm": 0.010594680905342102,
      "learning_rate": 4.818681318681319e-05,
      "loss": 0.7013,
      "step": 500
    },
    {
      "epoch": 0.38095238095238093,
      "grad_norm": 4.089939594268799,
      "learning_rate": 4.811355311355312e-05,
      "loss": 0.6498,
      "step": 520
    },
    {
      "epoch": 0.3956043956043956,
      "grad_norm": 13.740696907043457,
      "learning_rate": 4.804029304029304e-05,
      "loss": 0.7365,
      "step": 540
    },
    {
      "epoch": 0.41025641025641024,
      "grad_norm": 0.0014127000467851758,
      "learning_rate": 4.796703296703297e-05,
      "loss": 0.7471,
      "step": 560
    },
    {
      "epoch": 0.4249084249084249,
      "grad_norm": 13.840248107910156,
      "learning_rate": 4.78937728937729e-05,
      "loss": 0.9655,
      "step": 580
    },
    {
      "epoch": 0.43956043956043955,
      "grad_norm": 12.254070281982422,
      "learning_rate": 4.782051282051283e-05,
      "loss": 0.902,
      "step": 600
    },
    {
      "epoch": 0.4542124542124542,
      "grad_norm": 18.52558708190918,
      "learning_rate": 4.774725274725275e-05,
      "loss": 0.7406,
      "step": 620
    },
    {
      "epoch": 0.46886446886446886,
      "grad_norm": 1.0409804582595825,
      "learning_rate": 4.767399267399268e-05,
      "loss": 1.6894,
      "step": 640
    },
    {
      "epoch": 0.4835164835164835,
      "grad_norm": 0.5096887350082397,
      "learning_rate": 4.76007326007326e-05,
      "loss": 1.1644,
      "step": 660
    },
    {
      "epoch": 0.4981684981684982,
      "grad_norm": 36.13005828857422,
      "learning_rate": 4.752747252747253e-05,
      "loss": 1.5177,
      "step": 680
    },
    {
      "epoch": 0.5128205128205128,
      "grad_norm": 36.3756103515625,
      "learning_rate": 4.7454212454212456e-05,
      "loss": 1.5078,
      "step": 700
    },
    {
      "epoch": 0.5274725274725275,
      "grad_norm": 1.1332859992980957,
      "learning_rate": 4.738095238095238e-05,
      "loss": 1.2109,
      "step": 720
    },
    {
      "epoch": 0.5421245421245421,
      "grad_norm": 6.584702014923096,
      "learning_rate": 4.730769230769231e-05,
      "loss": 0.5398,
      "step": 740
    },
    {
      "epoch": 0.5567765567765568,
      "grad_norm": 37.829254150390625,
      "learning_rate": 4.7234432234432235e-05,
      "loss": 1.818,
      "step": 760
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 38.11442947387695,
      "learning_rate": 4.7161172161172164e-05,
      "loss": 1.9388,
      "step": 780
    },
    {
      "epoch": 0.5860805860805861,
      "grad_norm": 6.1774983406066895,
      "learning_rate": 4.708791208791209e-05,
      "loss": 1.0581,
      "step": 800
    },
    {
      "epoch": 0.6007326007326007,
      "grad_norm": 10.65705680847168,
      "learning_rate": 4.7014652014652014e-05,
      "loss": 0.627,
      "step": 820
    },
    {
      "epoch": 0.6153846153846154,
      "grad_norm": 30.396610260009766,
      "learning_rate": 4.694139194139194e-05,
      "loss": 1.5526,
      "step": 840
    },
    {
      "epoch": 0.63003663003663,
      "grad_norm": 37.13361740112305,
      "learning_rate": 4.686813186813187e-05,
      "loss": 0.8967,
      "step": 860
    },
    {
      "epoch": 0.6446886446886447,
      "grad_norm": 39.40074157714844,
      "learning_rate": 4.67948717948718e-05,
      "loss": 1.3935,
      "step": 880
    },
    {
      "epoch": 0.6593406593406593,
      "grad_norm": 34.246856689453125,
      "learning_rate": 4.672161172161172e-05,
      "loss": 1.1932,
      "step": 900
    },
    {
      "epoch": 0.673992673992674,
      "grad_norm": 36.8648796081543,
      "learning_rate": 4.664835164835165e-05,
      "loss": 1.2531,
      "step": 920
    },
    {
      "epoch": 0.6886446886446886,
      "grad_norm": 55.43345642089844,
      "learning_rate": 4.657509157509158e-05,
      "loss": 1.0101,
      "step": 940
    },
    {
      "epoch": 0.7032967032967034,
      "grad_norm": 22.256439208984375,
      "learning_rate": 4.650183150183151e-05,
      "loss": 1.6226,
      "step": 960
    },
    {
      "epoch": 0.717948717948718,
      "grad_norm": 38.78622817993164,
      "learning_rate": 4.642857142857143e-05,
      "loss": 1.1373,
      "step": 980
    },
    {
      "epoch": 0.7326007326007326,
      "grad_norm": 2.8306281566619873,
      "learning_rate": 4.635531135531136e-05,
      "loss": 1.5472,
      "step": 1000
    },
    {
      "epoch": 0.7472527472527473,
      "grad_norm": 0.202639639377594,
      "learning_rate": 4.6282051282051287e-05,
      "loss": 1.1128,
      "step": 1020
    },
    {
      "epoch": 0.7619047619047619,
      "grad_norm": 39.056861877441406,
      "learning_rate": 4.6208791208791215e-05,
      "loss": 1.4755,
      "step": 1040
    },
    {
      "epoch": 0.7765567765567766,
      "grad_norm": 8.457030296325684,
      "learning_rate": 4.6135531135531144e-05,
      "loss": 1.7182,
      "step": 1060
    },
    {
      "epoch": 0.7912087912087912,
      "grad_norm": 8.87135124206543,
      "learning_rate": 4.606227106227106e-05,
      "loss": 0.6419,
      "step": 1080
    },
    {
      "epoch": 0.8058608058608059,
      "grad_norm": 0.00969396810978651,
      "learning_rate": 4.598901098901099e-05,
      "loss": 1.2603,
      "step": 1100
    },
    {
      "epoch": 0.8205128205128205,
      "grad_norm": 34.20458984375,
      "learning_rate": 4.5915750915750916e-05,
      "loss": 0.7119,
      "step": 1120
    },
    {
      "epoch": 0.8351648351648352,
      "grad_norm": 0.13436363637447357,
      "learning_rate": 4.5842490842490844e-05,
      "loss": 1.793,
      "step": 1140
    },
    {
      "epoch": 0.8498168498168498,
      "grad_norm": 1.666268229484558,
      "learning_rate": 4.576923076923077e-05,
      "loss": 0.9246,
      "step": 1160
    },
    {
      "epoch": 0.8644688644688645,
      "grad_norm": 1.1750595569610596,
      "learning_rate": 4.5695970695970695e-05,
      "loss": 1.0647,
      "step": 1180
    },
    {
      "epoch": 0.8791208791208791,
      "grad_norm": 10.23388385772705,
      "learning_rate": 4.5622710622710623e-05,
      "loss": 2.0435,
      "step": 1200
    },
    {
      "epoch": 0.8937728937728938,
      "grad_norm": 0.5837065577507019,
      "learning_rate": 4.554945054945055e-05,
      "loss": 0.7134,
      "step": 1220
    },
    {
      "epoch": 0.9084249084249084,
      "grad_norm": 0.5379640460014343,
      "learning_rate": 4.547619047619048e-05,
      "loss": 1.2915,
      "step": 1240
    },
    {
      "epoch": 0.9230769230769231,
      "grad_norm": 1.0135012865066528,
      "learning_rate": 4.54029304029304e-05,
      "loss": 1.5846,
      "step": 1260
    },
    {
      "epoch": 0.9377289377289377,
      "grad_norm": 0.23500162363052368,
      "learning_rate": 4.532967032967033e-05,
      "loss": 0.723,
      "step": 1280
    },
    {
      "epoch": 0.9523809523809523,
      "grad_norm": 17.24457550048828,
      "learning_rate": 4.525641025641026e-05,
      "loss": 0.7657,
      "step": 1300
    },
    {
      "epoch": 0.967032967032967,
      "grad_norm": 4.83858585357666,
      "learning_rate": 4.518315018315019e-05,
      "loss": 0.6936,
      "step": 1320
    },
    {
      "epoch": 0.9816849816849816,
      "grad_norm": 0.7139630317687988,
      "learning_rate": 4.510989010989011e-05,
      "loss": 0.5749,
      "step": 1340
    },
    {
      "epoch": 0.9963369963369964,
      "grad_norm": 0.4039943218231201,
      "learning_rate": 4.503663003663004e-05,
      "loss": 0.4088,
      "step": 1360
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.7328767123287672,
      "eval_loss": 0.7919145226478577,
      "eval_runtime": 30.6233,
      "eval_samples_per_second": 9.535,
      "eval_steps_per_second": 1.208,
      "step": 1365
    },
    {
      "epoch": 1.010989010989011,
      "grad_norm": 10.993834495544434,
      "learning_rate": 4.496336996336997e-05,
      "loss": 1.1454,
      "step": 1380
    },
    {
      "epoch": 1.0256410256410255,
      "grad_norm": 32.83698272705078,
      "learning_rate": 4.4890109890109896e-05,
      "loss": 2.0364,
      "step": 1400
    },
    {
      "epoch": 1.0402930402930404,
      "grad_norm": 0.008791795931756496,
      "learning_rate": 4.481684981684982e-05,
      "loss": 0.6053,
      "step": 1420
    },
    {
      "epoch": 1.054945054945055,
      "grad_norm": 6.209752559661865,
      "learning_rate": 4.4743589743589746e-05,
      "loss": 0.5994,
      "step": 1440
    },
    {
      "epoch": 1.0695970695970696,
      "grad_norm": 24.387041091918945,
      "learning_rate": 4.4670329670329675e-05,
      "loss": 0.8753,
      "step": 1460
    },
    {
      "epoch": 1.0842490842490842,
      "grad_norm": 1.8849657773971558,
      "learning_rate": 4.45970695970696e-05,
      "loss": 0.8429,
      "step": 1480
    },
    {
      "epoch": 1.098901098901099,
      "grad_norm": 11.649588584899902,
      "learning_rate": 4.4523809523809525e-05,
      "loss": 0.5929,
      "step": 1500
    },
    {
      "epoch": 1.1135531135531136,
      "grad_norm": 54.71263122558594,
      "learning_rate": 4.445054945054945e-05,
      "loss": 0.5276,
      "step": 1520
    },
    {
      "epoch": 1.1282051282051282,
      "grad_norm": 0.007473767269402742,
      "learning_rate": 4.4377289377289375e-05,
      "loss": 0.4982,
      "step": 1540
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 38.2171745300293,
      "learning_rate": 4.4304029304029304e-05,
      "loss": 1.601,
      "step": 1560
    },
    {
      "epoch": 1.1575091575091574,
      "grad_norm": 62.399574279785156,
      "learning_rate": 4.423076923076923e-05,
      "loss": 0.367,
      "step": 1580
    },
    {
      "epoch": 1.1721611721611722,
      "grad_norm": 0.10493890941143036,
      "learning_rate": 4.415750915750916e-05,
      "loss": 1.9035,
      "step": 1600
    },
    {
      "epoch": 1.1868131868131868,
      "grad_norm": 0.019464101642370224,
      "learning_rate": 4.408424908424908e-05,
      "loss": 0.3474,
      "step": 1620
    },
    {
      "epoch": 1.2014652014652014,
      "grad_norm": 0.0039178356528282166,
      "learning_rate": 4.401098901098901e-05,
      "loss": 0.5574,
      "step": 1640
    },
    {
      "epoch": 1.2161172161172162,
      "grad_norm": 0.011957542039453983,
      "learning_rate": 4.393772893772894e-05,
      "loss": 0.6605,
      "step": 1660
    },
    {
      "epoch": 1.2307692307692308,
      "grad_norm": 0.0022226867731660604,
      "learning_rate": 4.386446886446887e-05,
      "loss": 0.9757,
      "step": 1680
    },
    {
      "epoch": 1.2454212454212454,
      "grad_norm": 0.0038918135687708855,
      "learning_rate": 4.379120879120879e-05,
      "loss": 0.4982,
      "step": 1700
    },
    {
      "epoch": 1.26007326007326,
      "grad_norm": 16.68226432800293,
      "learning_rate": 4.371794871794872e-05,
      "loss": 0.4787,
      "step": 1720
    },
    {
      "epoch": 1.2747252747252746,
      "grad_norm": 41.970741271972656,
      "learning_rate": 4.364468864468865e-05,
      "loss": 1.0312,
      "step": 1740
    },
    {
      "epoch": 1.2893772893772895,
      "grad_norm": 0.008095214143395424,
      "learning_rate": 4.3571428571428576e-05,
      "loss": 0.4164,
      "step": 1760
    },
    {
      "epoch": 1.304029304029304,
      "grad_norm": 1.1318141222000122,
      "learning_rate": 4.34981684981685e-05,
      "loss": 0.9059,
      "step": 1780
    },
    {
      "epoch": 1.3186813186813187,
      "grad_norm": 3.8613884449005127,
      "learning_rate": 4.342490842490843e-05,
      "loss": 1.2948,
      "step": 1800
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.11458896100521088,
      "learning_rate": 4.3351648351648355e-05,
      "loss": 0.9315,
      "step": 1820
    },
    {
      "epoch": 1.347985347985348,
      "grad_norm": 8.139902114868164,
      "learning_rate": 4.3278388278388284e-05,
      "loss": 0.7559,
      "step": 1840
    },
    {
      "epoch": 1.3626373626373627,
      "grad_norm": 0.042421068996191025,
      "learning_rate": 4.320512820512821e-05,
      "loss": 0.9041,
      "step": 1860
    },
    {
      "epoch": 1.3772893772893773,
      "grad_norm": 56.02604675292969,
      "learning_rate": 4.3131868131868134e-05,
      "loss": 0.5678,
      "step": 1880
    },
    {
      "epoch": 1.3919413919413919,
      "grad_norm": 0.0043878438882529736,
      "learning_rate": 4.305860805860806e-05,
      "loss": 0.9722,
      "step": 1900
    },
    {
      "epoch": 1.4065934065934065,
      "grad_norm": 47.384498596191406,
      "learning_rate": 4.298534798534799e-05,
      "loss": 1.0172,
      "step": 1920
    },
    {
      "epoch": 1.4212454212454213,
      "grad_norm": 49.901065826416016,
      "learning_rate": 4.291208791208791e-05,
      "loss": 1.3247,
      "step": 1940
    },
    {
      "epoch": 1.435897435897436,
      "grad_norm": 3.618140459060669,
      "learning_rate": 4.283882783882784e-05,
      "loss": 1.2618,
      "step": 1960
    },
    {
      "epoch": 1.4505494505494505,
      "grad_norm": 8.389519691467285,
      "learning_rate": 4.2765567765567764e-05,
      "loss": 1.74,
      "step": 1980
    },
    {
      "epoch": 1.4652014652014653,
      "grad_norm": 3.6713266372680664,
      "learning_rate": 4.269230769230769e-05,
      "loss": 0.3273,
      "step": 2000
    },
    {
      "epoch": 1.47985347985348,
      "grad_norm": 45.267948150634766,
      "learning_rate": 4.261904761904762e-05,
      "loss": 0.9658,
      "step": 2020
    },
    {
      "epoch": 1.4945054945054945,
      "grad_norm": 47.39130401611328,
      "learning_rate": 4.254578754578755e-05,
      "loss": 0.8404,
      "step": 2040
    },
    {
      "epoch": 1.5091575091575091,
      "grad_norm": 47.50275421142578,
      "learning_rate": 4.247252747252747e-05,
      "loss": 1.1645,
      "step": 2060
    },
    {
      "epoch": 1.5238095238095237,
      "grad_norm": 0.16881689429283142,
      "learning_rate": 4.23992673992674e-05,
      "loss": 0.8694,
      "step": 2080
    },
    {
      "epoch": 1.5384615384615383,
      "grad_norm": 48.15253448486328,
      "learning_rate": 4.232600732600733e-05,
      "loss": 0.5577,
      "step": 2100
    },
    {
      "epoch": 1.5531135531135531,
      "grad_norm": 2.1548125743865967,
      "learning_rate": 4.225274725274726e-05,
      "loss": 0.4076,
      "step": 2120
    },
    {
      "epoch": 1.5677655677655677,
      "grad_norm": 41.99747085571289,
      "learning_rate": 4.217948717948718e-05,
      "loss": 0.3807,
      "step": 2140
    },
    {
      "epoch": 1.5824175824175826,
      "grad_norm": 0.004832839593291283,
      "learning_rate": 4.210622710622711e-05,
      "loss": 0.2218,
      "step": 2160
    },
    {
      "epoch": 1.5970695970695972,
      "grad_norm": 0.1473255753517151,
      "learning_rate": 4.2032967032967036e-05,
      "loss": 0.3781,
      "step": 2180
    },
    {
      "epoch": 1.6117216117216118,
      "grad_norm": 0.21044720709323883,
      "learning_rate": 4.1959706959706964e-05,
      "loss": 0.7217,
      "step": 2200
    },
    {
      "epoch": 1.6263736263736264,
      "grad_norm": 0.7770096063613892,
      "learning_rate": 4.1886446886446886e-05,
      "loss": 0.7333,
      "step": 2220
    },
    {
      "epoch": 1.641025641025641,
      "grad_norm": 51.42171859741211,
      "learning_rate": 4.1813186813186815e-05,
      "loss": 0.8877,
      "step": 2240
    },
    {
      "epoch": 1.6556776556776556,
      "grad_norm": 1.0802788734436035,
      "learning_rate": 4.1739926739926743e-05,
      "loss": 1.0333,
      "step": 2260
    },
    {
      "epoch": 1.6703296703296702,
      "grad_norm": 0.015381813049316406,
      "learning_rate": 4.166666666666667e-05,
      "loss": 1.4864,
      "step": 2280
    },
    {
      "epoch": 1.684981684981685,
      "grad_norm": 0.0004958771169185638,
      "learning_rate": 4.15934065934066e-05,
      "loss": 0.4507,
      "step": 2300
    },
    {
      "epoch": 1.6996336996336996,
      "grad_norm": 1.5418614149093628,
      "learning_rate": 4.152014652014652e-05,
      "loss": 0.4121,
      "step": 2320
    },
    {
      "epoch": 1.7142857142857144,
      "grad_norm": 0.48339584469795227,
      "learning_rate": 4.144688644688645e-05,
      "loss": 0.4081,
      "step": 2340
    },
    {
      "epoch": 1.728937728937729,
      "grad_norm": 10.225020408630371,
      "learning_rate": 4.137362637362637e-05,
      "loss": 0.7718,
      "step": 2360
    },
    {
      "epoch": 1.7435897435897436,
      "grad_norm": 46.12253952026367,
      "learning_rate": 4.13003663003663e-05,
      "loss": 1.2991,
      "step": 2380
    },
    {
      "epoch": 1.7582417582417582,
      "grad_norm": 0.3936282992362976,
      "learning_rate": 4.122710622710623e-05,
      "loss": 0.6371,
      "step": 2400
    },
    {
      "epoch": 1.7728937728937728,
      "grad_norm": 0.17120811343193054,
      "learning_rate": 4.115384615384615e-05,
      "loss": 0.526,
      "step": 2420
    },
    {
      "epoch": 1.7875457875457874,
      "grad_norm": 56.57362747192383,
      "learning_rate": 4.108058608058608e-05,
      "loss": 0.6068,
      "step": 2440
    },
    {
      "epoch": 1.8021978021978022,
      "grad_norm": 47.873382568359375,
      "learning_rate": 4.100732600732601e-05,
      "loss": 1.3729,
      "step": 2460
    },
    {
      "epoch": 1.8168498168498168,
      "grad_norm": 0.011669516563415527,
      "learning_rate": 4.093406593406594e-05,
      "loss": 0.3885,
      "step": 2480
    },
    {
      "epoch": 1.8315018315018317,
      "grad_norm": 0.0008985654567368329,
      "learning_rate": 4.086080586080586e-05,
      "loss": 0.2213,
      "step": 2500
    },
    {
      "epoch": 1.8461538461538463,
      "grad_norm": 0.0004855594888795167,
      "learning_rate": 4.078754578754579e-05,
      "loss": 0.4464,
      "step": 2520
    },
    {
      "epoch": 1.8608058608058609,
      "grad_norm": 75.99504852294922,
      "learning_rate": 4.0714285714285717e-05,
      "loss": 0.6444,
      "step": 2540
    },
    {
      "epoch": 1.8754578754578755,
      "grad_norm": 2.0176730155944824,
      "learning_rate": 4.064468864468864e-05,
      "loss": 1.1854,
      "step": 2560
    },
    {
      "epoch": 1.89010989010989,
      "grad_norm": 37.1988525390625,
      "learning_rate": 4.057142857142857e-05,
      "loss": 0.6472,
      "step": 2580
    },
    {
      "epoch": 1.9047619047619047,
      "grad_norm": 52.04927444458008,
      "learning_rate": 4.04981684981685e-05,
      "loss": 0.6963,
      "step": 2600
    },
    {
      "epoch": 1.9194139194139193,
      "grad_norm": 0.24837560951709747,
      "learning_rate": 4.042490842490843e-05,
      "loss": 0.6885,
      "step": 2620
    },
    {
      "epoch": 1.934065934065934,
      "grad_norm": 0.008687284775078297,
      "learning_rate": 4.035164835164835e-05,
      "loss": 0.7936,
      "step": 2640
    },
    {
      "epoch": 1.9487179487179487,
      "grad_norm": 1.155282735824585,
      "learning_rate": 4.027838827838828e-05,
      "loss": 0.2469,
      "step": 2660
    },
    {
      "epoch": 1.9633699633699635,
      "grad_norm": 0.0002647249784786254,
      "learning_rate": 4.020512820512821e-05,
      "loss": 0.6521,
      "step": 2680
    },
    {
      "epoch": 1.978021978021978,
      "grad_norm": 47.927825927734375,
      "learning_rate": 4.0131868131868136e-05,
      "loss": 1.3161,
      "step": 2700
    },
    {
      "epoch": 1.9926739926739927,
      "grad_norm": 0.006751004606485367,
      "learning_rate": 4.005860805860806e-05,
      "loss": 0.9341,
      "step": 2720
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.7671232876712328,
      "eval_loss": 0.9800509214401245,
      "eval_runtime": 30.056,
      "eval_samples_per_second": 9.715,
      "eval_steps_per_second": 1.231,
      "step": 2730
    },
    {
      "epoch": 2.0073260073260073,
      "grad_norm": 46.61191177368164,
      "learning_rate": 3.9985347985347986e-05,
      "loss": 1.0025,
      "step": 2740
    },
    {
      "epoch": 2.021978021978022,
      "grad_norm": 47.73088073730469,
      "learning_rate": 3.9912087912087915e-05,
      "loss": 0.6381,
      "step": 2760
    },
    {
      "epoch": 2.0366300366300365,
      "grad_norm": 94.33724212646484,
      "learning_rate": 3.9838827838827843e-05,
      "loss": 0.3359,
      "step": 2780
    },
    {
      "epoch": 2.051282051282051,
      "grad_norm": 54.21308517456055,
      "learning_rate": 3.9765567765567765e-05,
      "loss": 0.6206,
      "step": 2800
    },
    {
      "epoch": 2.065934065934066,
      "grad_norm": 0.020686503499746323,
      "learning_rate": 3.9692307692307694e-05,
      "loss": 0.1946,
      "step": 2820
    },
    {
      "epoch": 2.0805860805860807,
      "grad_norm": 0.053057193756103516,
      "learning_rate": 3.961904761904762e-05,
      "loss": 0.7898,
      "step": 2840
    },
    {
      "epoch": 2.0952380952380953,
      "grad_norm": 17.13546371459961,
      "learning_rate": 3.954578754578755e-05,
      "loss": 0.4452,
      "step": 2860
    },
    {
      "epoch": 2.10989010989011,
      "grad_norm": 0.0012474708491936326,
      "learning_rate": 3.947252747252748e-05,
      "loss": 0.1392,
      "step": 2880
    },
    {
      "epoch": 2.1245421245421245,
      "grad_norm": 0.06859268993139267,
      "learning_rate": 3.93992673992674e-05,
      "loss": 0.0572,
      "step": 2900
    },
    {
      "epoch": 2.139194139194139,
      "grad_norm": 0.39437711238861084,
      "learning_rate": 3.932600732600733e-05,
      "loss": 0.1067,
      "step": 2920
    },
    {
      "epoch": 2.1538461538461537,
      "grad_norm": 1.1829299926757812,
      "learning_rate": 3.925274725274726e-05,
      "loss": 0.537,
      "step": 2940
    },
    {
      "epoch": 2.1684981684981683,
      "grad_norm": 21.561189651489258,
      "learning_rate": 3.917948717948718e-05,
      "loss": 0.9747,
      "step": 2960
    },
    {
      "epoch": 2.183150183150183,
      "grad_norm": 0.00015093028196133673,
      "learning_rate": 3.910622710622711e-05,
      "loss": 0.7215,
      "step": 2980
    },
    {
      "epoch": 2.197802197802198,
      "grad_norm": 0.0006520894821733236,
      "learning_rate": 3.903296703296703e-05,
      "loss": 0.2869,
      "step": 3000
    },
    {
      "epoch": 2.2124542124542126,
      "grad_norm": 6.169074913486838e-05,
      "learning_rate": 3.895970695970696e-05,
      "loss": 1.1395,
      "step": 3020
    },
    {
      "epoch": 2.227106227106227,
      "grad_norm": 8.102608990157023e-05,
      "learning_rate": 3.888644688644689e-05,
      "loss": 0.2899,
      "step": 3040
    },
    {
      "epoch": 2.241758241758242,
      "grad_norm": 0.000685867213178426,
      "learning_rate": 3.8813186813186817e-05,
      "loss": 0.6301,
      "step": 3060
    },
    {
      "epoch": 2.2564102564102564,
      "grad_norm": 6.812795618316159e-05,
      "learning_rate": 3.873992673992674e-05,
      "loss": 0.9109,
      "step": 3080
    },
    {
      "epoch": 2.271062271062271,
      "grad_norm": 65.36656188964844,
      "learning_rate": 3.866666666666667e-05,
      "loss": 0.3911,
      "step": 3100
    },
    {
      "epoch": 2.2857142857142856,
      "grad_norm": 33.53612518310547,
      "learning_rate": 3.8593406593406595e-05,
      "loss": 0.2583,
      "step": 3120
    },
    {
      "epoch": 2.3003663003663,
      "grad_norm": 0.43566423654556274,
      "learning_rate": 3.8520146520146524e-05,
      "loss": 0.7963,
      "step": 3140
    },
    {
      "epoch": 2.315018315018315,
      "grad_norm": 0.05603724345564842,
      "learning_rate": 3.8446886446886446e-05,
      "loss": 0.4243,
      "step": 3160
    },
    {
      "epoch": 2.32967032967033,
      "grad_norm": 0.0057182032614946365,
      "learning_rate": 3.8373626373626374e-05,
      "loss": 0.3618,
      "step": 3180
    },
    {
      "epoch": 2.3443223443223444,
      "grad_norm": 0.006601232569664717,
      "learning_rate": 3.83003663003663e-05,
      "loss": 0.8687,
      "step": 3200
    },
    {
      "epoch": 2.358974358974359,
      "grad_norm": 0.0031544859521090984,
      "learning_rate": 3.822710622710623e-05,
      "loss": 0.0962,
      "step": 3220
    },
    {
      "epoch": 2.3736263736263736,
      "grad_norm": 0.3184678256511688,
      "learning_rate": 3.8153846153846153e-05,
      "loss": 1.0728,
      "step": 3240
    },
    {
      "epoch": 2.3882783882783882,
      "grad_norm": 0.5805619955062866,
      "learning_rate": 3.808058608058608e-05,
      "loss": 0.6798,
      "step": 3260
    },
    {
      "epoch": 2.402930402930403,
      "grad_norm": 0.014856583438813686,
      "learning_rate": 3.800732600732601e-05,
      "loss": 0.2609,
      "step": 3280
    },
    {
      "epoch": 2.4175824175824174,
      "grad_norm": 0.010517274960875511,
      "learning_rate": 3.793406593406594e-05,
      "loss": 0.3281,
      "step": 3300
    },
    {
      "epoch": 2.4322344322344325,
      "grad_norm": 17.405555725097656,
      "learning_rate": 3.786080586080587e-05,
      "loss": 0.0607,
      "step": 3320
    },
    {
      "epoch": 2.446886446886447,
      "grad_norm": 0.0024443697184324265,
      "learning_rate": 3.778754578754579e-05,
      "loss": 0.1104,
      "step": 3340
    },
    {
      "epoch": 2.4615384615384617,
      "grad_norm": 0.2145785540342331,
      "learning_rate": 3.771428571428572e-05,
      "loss": 0.6422,
      "step": 3360
    },
    {
      "epoch": 2.4761904761904763,
      "grad_norm": 4.317418098449707,
      "learning_rate": 3.764102564102564e-05,
      "loss": 0.5309,
      "step": 3380
    },
    {
      "epoch": 2.490842490842491,
      "grad_norm": 0.008890482597053051,
      "learning_rate": 3.756776556776557e-05,
      "loss": 1.3354,
      "step": 3400
    },
    {
      "epoch": 2.5054945054945055,
      "grad_norm": 0.03590291738510132,
      "learning_rate": 3.74945054945055e-05,
      "loss": 0.7197,
      "step": 3420
    },
    {
      "epoch": 2.52014652014652,
      "grad_norm": 0.13917331397533417,
      "learning_rate": 3.742124542124542e-05,
      "loss": 0.5354,
      "step": 3440
    },
    {
      "epoch": 2.5347985347985347,
      "grad_norm": 2.8041782570653595e-05,
      "learning_rate": 3.734798534798535e-05,
      "loss": 0.5166,
      "step": 3460
    },
    {
      "epoch": 2.5494505494505493,
      "grad_norm": 0.011242503300309181,
      "learning_rate": 3.7274725274725276e-05,
      "loss": 0.5574,
      "step": 3480
    },
    {
      "epoch": 2.564102564102564,
      "grad_norm": 0.22284525632858276,
      "learning_rate": 3.7201465201465205e-05,
      "loss": 0.0434,
      "step": 3500
    },
    {
      "epoch": 2.578754578754579,
      "grad_norm": 14.110017776489258,
      "learning_rate": 3.7128205128205126e-05,
      "loss": 0.2529,
      "step": 3520
    },
    {
      "epoch": 2.5934065934065935,
      "grad_norm": 0.00017811775614973158,
      "learning_rate": 3.7054945054945055e-05,
      "loss": 0.1019,
      "step": 3540
    },
    {
      "epoch": 2.608058608058608,
      "grad_norm": 0.03377379849553108,
      "learning_rate": 3.6981684981684984e-05,
      "loss": 0.5039,
      "step": 3560
    },
    {
      "epoch": 2.6227106227106227,
      "grad_norm": 0.0024217627942562103,
      "learning_rate": 3.690842490842491e-05,
      "loss": 0.4966,
      "step": 3580
    },
    {
      "epoch": 2.6373626373626373,
      "grad_norm": 16.474578857421875,
      "learning_rate": 3.6835164835164834e-05,
      "loss": 0.5044,
      "step": 3600
    },
    {
      "epoch": 2.652014652014652,
      "grad_norm": 0.0011118261609226465,
      "learning_rate": 3.676190476190476e-05,
      "loss": 0.5897,
      "step": 3620
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 3.871938943862915,
      "learning_rate": 3.668864468864469e-05,
      "loss": 0.1033,
      "step": 3640
    },
    {
      "epoch": 2.6813186813186816,
      "grad_norm": 0.0012167079839855433,
      "learning_rate": 3.661538461538462e-05,
      "loss": 0.1129,
      "step": 3660
    },
    {
      "epoch": 2.695970695970696,
      "grad_norm": 0.05292464792728424,
      "learning_rate": 3.654212454212455e-05,
      "loss": 0.6294,
      "step": 3680
    },
    {
      "epoch": 2.7106227106227108,
      "grad_norm": 5.239331585471518e-05,
      "learning_rate": 3.646886446886447e-05,
      "loss": 0.519,
      "step": 3700
    },
    {
      "epoch": 2.7252747252747254,
      "grad_norm": 0.003960806410759687,
      "learning_rate": 3.63956043956044e-05,
      "loss": 0.5154,
      "step": 3720
    },
    {
      "epoch": 2.73992673992674,
      "grad_norm": 7.178754458436742e-05,
      "learning_rate": 3.632234432234433e-05,
      "loss": 1.3512,
      "step": 3740
    },
    {
      "epoch": 2.7545787545787546,
      "grad_norm": 0.0419740192592144,
      "learning_rate": 3.6249084249084256e-05,
      "loss": 0.94,
      "step": 3760
    },
    {
      "epoch": 2.769230769230769,
      "grad_norm": 0.00043715734500437975,
      "learning_rate": 3.617582417582418e-05,
      "loss": 0.119,
      "step": 3780
    },
    {
      "epoch": 2.7838827838827838,
      "grad_norm": 0.5314416885375977,
      "learning_rate": 3.6102564102564106e-05,
      "loss": 0.4003,
      "step": 3800
    },
    {
      "epoch": 2.7985347985347984,
      "grad_norm": 45.21880340576172,
      "learning_rate": 3.602930402930403e-05,
      "loss": 0.249,
      "step": 3820
    },
    {
      "epoch": 2.813186813186813,
      "grad_norm": 0.015885669738054276,
      "learning_rate": 3.595604395604396e-05,
      "loss": 0.894,
      "step": 3840
    },
    {
      "epoch": 2.8278388278388276,
      "grad_norm": 0.00014973955694586039,
      "learning_rate": 3.5882783882783885e-05,
      "loss": 0.2163,
      "step": 3860
    },
    {
      "epoch": 2.8424908424908426,
      "grad_norm": 0.0991942286491394,
      "learning_rate": 3.580952380952381e-05,
      "loss": 0.4192,
      "step": 3880
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 0.003748080227524042,
      "learning_rate": 3.5736263736263736e-05,
      "loss": 0.4587,
      "step": 3900
    },
    {
      "epoch": 2.871794871794872,
      "grad_norm": 0.004745875019580126,
      "learning_rate": 3.5663003663003664e-05,
      "loss": 0.6021,
      "step": 3920
    },
    {
      "epoch": 2.8864468864468864,
      "grad_norm": 96.91908264160156,
      "learning_rate": 3.558974358974359e-05,
      "loss": 0.7544,
      "step": 3940
    },
    {
      "epoch": 2.901098901098901,
      "grad_norm": 44.19925308227539,
      "learning_rate": 3.5516483516483515e-05,
      "loss": 0.8857,
      "step": 3960
    },
    {
      "epoch": 2.9157509157509156,
      "grad_norm": 0.014251232147216797,
      "learning_rate": 3.544322344322344e-05,
      "loss": 0.6764,
      "step": 3980
    },
    {
      "epoch": 2.9304029304029307,
      "grad_norm": 74.45452117919922,
      "learning_rate": 3.536996336996337e-05,
      "loss": 0.7262,
      "step": 4000
    },
    {
      "epoch": 2.9450549450549453,
      "grad_norm": 174.93862915039062,
      "learning_rate": 3.52967032967033e-05,
      "loss": 0.3949,
      "step": 4020
    },
    {
      "epoch": 2.95970695970696,
      "grad_norm": 6.556238651275635,
      "learning_rate": 3.522344322344322e-05,
      "loss": 0.4229,
      "step": 4040
    },
    {
      "epoch": 2.9743589743589745,
      "grad_norm": 0.4353925883769989,
      "learning_rate": 3.515018315018315e-05,
      "loss": 0.242,
      "step": 4060
    },
    {
      "epoch": 2.989010989010989,
      "grad_norm": 0.2651078402996063,
      "learning_rate": 3.507692307692308e-05,
      "loss": 0.8345,
      "step": 4080
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.7773972602739726,
      "eval_loss": 1.2617852687835693,
      "eval_runtime": 29.1612,
      "eval_samples_per_second": 10.013,
      "eval_steps_per_second": 1.269,
      "step": 4095
    },
    {
      "epoch": 3.0036630036630036,
      "grad_norm": 0.0017805055249482393,
      "learning_rate": 3.500366300366301e-05,
      "loss": 0.1209,
      "step": 4100
    },
    {
      "epoch": 3.0183150183150182,
      "grad_norm": 0.21132723987102509,
      "learning_rate": 3.4930402930402937e-05,
      "loss": 0.3506,
      "step": 4120
    },
    {
      "epoch": 3.032967032967033,
      "grad_norm": 0.003785414155572653,
      "learning_rate": 3.485714285714286e-05,
      "loss": 1.0597,
      "step": 4140
    },
    {
      "epoch": 3.0476190476190474,
      "grad_norm": 60.59720993041992,
      "learning_rate": 3.478388278388279e-05,
      "loss": 0.0348,
      "step": 4160
    },
    {
      "epoch": 3.062271062271062,
      "grad_norm": 0.0003736163198482245,
      "learning_rate": 3.4710622710622716e-05,
      "loss": 0.5237,
      "step": 4180
    },
    {
      "epoch": 3.076923076923077,
      "grad_norm": 0.041001223027706146,
      "learning_rate": 3.4637362637362644e-05,
      "loss": 0.0065,
      "step": 4200
    },
    {
      "epoch": 3.0915750915750917,
      "grad_norm": 0.00012976833386346698,
      "learning_rate": 3.4564102564102566e-05,
      "loss": 0.4394,
      "step": 4220
    },
    {
      "epoch": 3.1062271062271063,
      "grad_norm": 0.0012896821135655046,
      "learning_rate": 3.449084249084249e-05,
      "loss": 0.4403,
      "step": 4240
    },
    {
      "epoch": 3.120879120879121,
      "grad_norm": 0.00017015065532177687,
      "learning_rate": 3.4417582417582416e-05,
      "loss": 0.3579,
      "step": 4260
    },
    {
      "epoch": 3.1355311355311355,
      "grad_norm": 70.0797348022461,
      "learning_rate": 3.4344322344322345e-05,
      "loss": 0.674,
      "step": 4280
    },
    {
      "epoch": 3.15018315018315,
      "grad_norm": 0.02042064256966114,
      "learning_rate": 3.4271062271062273e-05,
      "loss": 0.1419,
      "step": 4300
    },
    {
      "epoch": 3.1648351648351647,
      "grad_norm": 0.3358951210975647,
      "learning_rate": 3.4197802197802195e-05,
      "loss": 0.2876,
      "step": 4320
    },
    {
      "epoch": 3.1794871794871793,
      "grad_norm": 0.0007863157661631703,
      "learning_rate": 3.4124542124542124e-05,
      "loss": 0.7093,
      "step": 4340
    },
    {
      "epoch": 3.1941391941391943,
      "grad_norm": 0.007441221736371517,
      "learning_rate": 3.405128205128205e-05,
      "loss": 0.2072,
      "step": 4360
    },
    {
      "epoch": 3.208791208791209,
      "grad_norm": 2.7532609237823635e-05,
      "learning_rate": 3.397802197802198e-05,
      "loss": 0.1165,
      "step": 4380
    },
    {
      "epoch": 3.2234432234432235,
      "grad_norm": 1.464993147237692e-05,
      "learning_rate": 3.39047619047619e-05,
      "loss": 0.5954,
      "step": 4400
    },
    {
      "epoch": 3.238095238095238,
      "grad_norm": 0.0018327648285776377,
      "learning_rate": 3.383150183150183e-05,
      "loss": 0.4551,
      "step": 4420
    },
    {
      "epoch": 3.2527472527472527,
      "grad_norm": 0.000290752766886726,
      "learning_rate": 3.375824175824176e-05,
      "loss": 0.1404,
      "step": 4440
    },
    {
      "epoch": 3.2673992673992673,
      "grad_norm": 3.5835139751434326,
      "learning_rate": 3.368498168498169e-05,
      "loss": 0.4287,
      "step": 4460
    },
    {
      "epoch": 3.282051282051282,
      "grad_norm": 0.002624262124300003,
      "learning_rate": 3.361172161172162e-05,
      "loss": 0.2823,
      "step": 4480
    },
    {
      "epoch": 3.2967032967032965,
      "grad_norm": 2.8668297090916894e-05,
      "learning_rate": 3.353846153846154e-05,
      "loss": 0.2274,
      "step": 4500
    },
    {
      "epoch": 3.311355311355311,
      "grad_norm": 0.055493734776973724,
      "learning_rate": 3.346520146520147e-05,
      "loss": 0.0531,
      "step": 4520
    },
    {
      "epoch": 3.326007326007326,
      "grad_norm": 0.009736274369060993,
      "learning_rate": 3.3391941391941396e-05,
      "loss": 0.0555,
      "step": 4540
    },
    {
      "epoch": 3.340659340659341,
      "grad_norm": 0.00011236112186452374,
      "learning_rate": 3.3318681318681325e-05,
      "loss": 0.4026,
      "step": 4560
    },
    {
      "epoch": 3.3553113553113554,
      "grad_norm": 0.00013423303607851267,
      "learning_rate": 3.3245421245421247e-05,
      "loss": 0.3896,
      "step": 4580
    },
    {
      "epoch": 3.36996336996337,
      "grad_norm": 0.09272705763578415,
      "learning_rate": 3.3172161172161175e-05,
      "loss": 0.0277,
      "step": 4600
    },
    {
      "epoch": 3.3846153846153846,
      "grad_norm": 0.049832671880722046,
      "learning_rate": 3.3098901098901104e-05,
      "loss": 0.8966,
      "step": 4620
    },
    {
      "epoch": 3.399267399267399,
      "grad_norm": 0.0001575790229253471,
      "learning_rate": 3.302564102564103e-05,
      "loss": 0.7663,
      "step": 4640
    },
    {
      "epoch": 3.413919413919414,
      "grad_norm": 7.983934483490884e-05,
      "learning_rate": 3.2952380952380954e-05,
      "loss": 0.0449,
      "step": 4660
    },
    {
      "epoch": 3.4285714285714284,
      "grad_norm": 4.399776935577393,
      "learning_rate": 3.2879120879120876e-05,
      "loss": 0.4991,
      "step": 4680
    },
    {
      "epoch": 3.4432234432234434,
      "grad_norm": 0.0004221805720590055,
      "learning_rate": 3.2805860805860804e-05,
      "loss": 0.0455,
      "step": 4700
    },
    {
      "epoch": 3.457875457875458,
      "grad_norm": 114.67924499511719,
      "learning_rate": 3.273260073260073e-05,
      "loss": 0.8808,
      "step": 4720
    },
    {
      "epoch": 3.4725274725274726,
      "grad_norm": 5.5581109336344525e-05,
      "learning_rate": 3.265934065934066e-05,
      "loss": 0.2454,
      "step": 4740
    },
    {
      "epoch": 3.4871794871794872,
      "grad_norm": 5.313420295715332,
      "learning_rate": 3.2586080586080583e-05,
      "loss": 0.1546,
      "step": 4760
    },
    {
      "epoch": 3.501831501831502,
      "grad_norm": 0.11996039003133774,
      "learning_rate": 3.251282051282051e-05,
      "loss": 0.9379,
      "step": 4780
    },
    {
      "epoch": 3.5164835164835164,
      "grad_norm": 0.0002945518062915653,
      "learning_rate": 3.243956043956044e-05,
      "loss": 0.1381,
      "step": 4800
    },
    {
      "epoch": 3.531135531135531,
      "grad_norm": 47.07066345214844,
      "learning_rate": 3.236630036630037e-05,
      "loss": 0.2793,
      "step": 4820
    },
    {
      "epoch": 3.5457875457875456,
      "grad_norm": 0.011760533787310123,
      "learning_rate": 3.229304029304029e-05,
      "loss": 0.1617,
      "step": 4840
    },
    {
      "epoch": 3.5604395604395602,
      "grad_norm": 0.0037407074123620987,
      "learning_rate": 3.221978021978022e-05,
      "loss": 0.4247,
      "step": 4860
    },
    {
      "epoch": 3.575091575091575,
      "grad_norm": 0.0011057429946959019,
      "learning_rate": 3.215018315018315e-05,
      "loss": 0.6561,
      "step": 4880
    },
    {
      "epoch": 3.58974358974359,
      "grad_norm": 10.799092292785645,
      "learning_rate": 3.2076923076923074e-05,
      "loss": 0.8687,
      "step": 4900
    },
    {
      "epoch": 3.6043956043956045,
      "grad_norm": 0.003625627839937806,
      "learning_rate": 3.2003663003663e-05,
      "loss": 0.4064,
      "step": 4920
    },
    {
      "epoch": 3.619047619047619,
      "grad_norm": 0.008336481638252735,
      "learning_rate": 3.193040293040293e-05,
      "loss": 0.4628,
      "step": 4940
    },
    {
      "epoch": 3.6336996336996337,
      "grad_norm": 0.007940417155623436,
      "learning_rate": 3.185714285714286e-05,
      "loss": 0.4249,
      "step": 4960
    },
    {
      "epoch": 3.6483516483516483,
      "grad_norm": 0.0028622497338801622,
      "learning_rate": 3.178388278388278e-05,
      "loss": 0.1656,
      "step": 4980
    },
    {
      "epoch": 3.663003663003663,
      "grad_norm": 9.776187653187662e-05,
      "learning_rate": 3.171062271062271e-05,
      "loss": 0.0691,
      "step": 5000
    },
    {
      "epoch": 3.677655677655678,
      "grad_norm": 0.01906091719865799,
      "learning_rate": 3.163736263736264e-05,
      "loss": 0.1937,
      "step": 5020
    },
    {
      "epoch": 3.6923076923076925,
      "grad_norm": 2.5169725631712936e-05,
      "learning_rate": 3.156410256410257e-05,
      "loss": 0.573,
      "step": 5040
    },
    {
      "epoch": 3.706959706959707,
      "grad_norm": 0.0006854665698483586,
      "learning_rate": 3.149084249084249e-05,
      "loss": 0.3141,
      "step": 5060
    },
    {
      "epoch": 3.7216117216117217,
      "grad_norm": 5.735578088206239e-05,
      "learning_rate": 3.141758241758242e-05,
      "loss": 0.298,
      "step": 5080
    },
    {
      "epoch": 3.7362637362637363,
      "grad_norm": 0.0009009534260258079,
      "learning_rate": 3.1344322344322346e-05,
      "loss": 0.7629,
      "step": 5100
    },
    {
      "epoch": 3.750915750915751,
      "grad_norm": 0.001974593847990036,
      "learning_rate": 3.1271062271062275e-05,
      "loss": 0.8552,
      "step": 5120
    },
    {
      "epoch": 3.7655677655677655,
      "grad_norm": 0.0009070546948350966,
      "learning_rate": 3.1197802197802204e-05,
      "loss": 0.3269,
      "step": 5140
    },
    {
      "epoch": 3.78021978021978,
      "grad_norm": 0.024916300550103188,
      "learning_rate": 3.1124542124542125e-05,
      "loss": 0.0486,
      "step": 5160
    },
    {
      "epoch": 3.7948717948717947,
      "grad_norm": 0.00020751568081323057,
      "learning_rate": 3.1051282051282054e-05,
      "loss": 0.4795,
      "step": 5180
    },
    {
      "epoch": 3.8095238095238093,
      "grad_norm": 0.00023786615929566324,
      "learning_rate": 3.097802197802198e-05,
      "loss": 0.1445,
      "step": 5200
    },
    {
      "epoch": 3.824175824175824,
      "grad_norm": 0.2668071985244751,
      "learning_rate": 3.090476190476191e-05,
      "loss": 0.1225,
      "step": 5220
    },
    {
      "epoch": 3.838827838827839,
      "grad_norm": 0.03726230934262276,
      "learning_rate": 3.083150183150183e-05,
      "loss": 0.5868,
      "step": 5240
    },
    {
      "epoch": 3.8534798534798536,
      "grad_norm": 7.077820301055908,
      "learning_rate": 3.0758241758241755e-05,
      "loss": 0.2559,
      "step": 5260
    },
    {
      "epoch": 3.868131868131868,
      "grad_norm": 61.40232467651367,
      "learning_rate": 3.0684981684981683e-05,
      "loss": 1.3891,
      "step": 5280
    },
    {
      "epoch": 3.8827838827838828,
      "grad_norm": 0.005994865670800209,
      "learning_rate": 3.061172161172161e-05,
      "loss": 0.8235,
      "step": 5300
    },
    {
      "epoch": 3.8974358974358974,
      "grad_norm": 0.0001261992729268968,
      "learning_rate": 3.053846153846154e-05,
      "loss": 0.1478,
      "step": 5320
    },
    {
      "epoch": 3.912087912087912,
      "grad_norm": 0.000703565776348114,
      "learning_rate": 3.0465201465201466e-05,
      "loss": 0.261,
      "step": 5340
    },
    {
      "epoch": 3.926739926739927,
      "grad_norm": 0.5848084688186646,
      "learning_rate": 3.039194139194139e-05,
      "loss": 0.7373,
      "step": 5360
    },
    {
      "epoch": 3.9413919413919416,
      "grad_norm": 0.08507070690393448,
      "learning_rate": 3.031868131868132e-05,
      "loss": 0.0405,
      "step": 5380
    },
    {
      "epoch": 3.956043956043956,
      "grad_norm": 0.00023878297361079603,
      "learning_rate": 3.0245421245421245e-05,
      "loss": 0.371,
      "step": 5400
    },
    {
      "epoch": 3.970695970695971,
      "grad_norm": 0.11725319176912308,
      "learning_rate": 3.0172161172161173e-05,
      "loss": 0.5684,
      "step": 5420
    },
    {
      "epoch": 3.9853479853479854,
      "grad_norm": 12.104135513305664,
      "learning_rate": 3.00989010989011e-05,
      "loss": 0.3587,
      "step": 5440
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.25919291377067566,
      "learning_rate": 3.0025641025641027e-05,
      "loss": 0.3446,
      "step": 5460
    },
    {
      "epoch": 4.0,
      "eval_accuracy": 0.8698630136986302,
      "eval_loss": 0.5250003933906555,
      "eval_runtime": 28.9978,
      "eval_samples_per_second": 10.07,
      "eval_steps_per_second": 1.276,
      "step": 5460
    }
  ],
  "logging_steps": 20,
  "max_steps": 13650,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 3,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.913505722396836e+19,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
